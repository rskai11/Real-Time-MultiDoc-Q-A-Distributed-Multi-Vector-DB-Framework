{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Parallel Computing Libraries\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rouna\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "global embeddings\n",
    "global text_Splitter\n",
    "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "text_Splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb.config\n",
    "\n",
    "\n",
    "def Vector_DB_Retriever(file_name):\n",
    "    loader=PyPDFLoader(f\"Data/{file_name}\")\n",
    "    file=loader.load()\n",
    "    doc=text_Splitter.split_documents(file)\n",
    "    client_settings=chromadb.config.Settings(\n",
    "        is_persistent=False,\n",
    "        anonymized_telemetry=False,\n",
    "    )\n",
    "    Doc=Chroma.from_documents(doc,embeddings,client_settings=client_settings,\n",
    "                              collection_name=file_name.split('.')[0],\n",
    "                              collection_metadata={'hnsw':'cosine'})\n",
    "    retriever_Doc=Doc.as_retriever(search_type='mmr',search_kwargs={'k':10,\"include_metadata\":True})\n",
    "    return retriever_Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=[s for s in os.listdir('.//Data') if s.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(files)) as excecutor:\n",
    "    results=list(excecutor.map(Vector_DB_Retriever,files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MGR=MergerRetriever(retrievers=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers import ContextualCompressionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rouna\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c926561fbdc140709bde6adae49e7aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/799 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rouna\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rouna\\.cache\\huggingface\\hub\\models--BAAI--bge-reranker-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa78cd3d7ee4b769243c0f59b683b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4e904e08c04f9db096059fc19cfd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c8c28eb86a4441aa5a746b9116e2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7422c8d42dc54063902ef76f9c6304a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17deb098348f49ce8791559f48761695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_encoder_model=HuggingFaceCrossEncoder(model_name='BAAI/bge-reranker-base')\n",
    "rerank_compressor=CrossEncoderReranker(model=cross_encoder_model,top_n=5)\n",
    "pipeline=DocumentCompressorPipeline(transformers=[rerank_compressor])\n",
    "Final_Retriever=ContextualCompressionRetriever(base_compressor=pipeline,base_retriever=MGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Give me architecture of Comparison between GPT3 and LLAMA2\"\n",
    "context=Final_Retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyfine-tunedtoalignwithhuman', metadata={'page': 2, 'source': 'Data/LLAMA_2.pdf'}),\n",
       " Document(page_content='distribution, recovering strong performance in the few-shot setting.\\nOn Natural Questions (NQs) GPT-3 achieves 14.6% in the zero-shot setting, 23.0% in the one-shot setting, and 29.9% in\\nthe few-shot setting, compared to 36.6% for ﬁne-tuned T5 11B+SSM. Similar to WebQS, the large gain from zero-shot\\nto few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to', metadata={'page': 13, 'source': 'Data/GPT3.pdf'}),\n",
       " Document(page_content='Therefore,basedontheablationresultsandeaseofscalinginference,forthe34Band70B Llama 2 models\\nwe chose to use GQA instead of MQA.\\nFigure 24 shows how inference speed changed for the 30B GQA and MQA ablation models compared to the\\nMHAbaseline,inanexperimentusing8x80GiBA100swithtensorparallelism. Intheserunswesimply\\nduplicated the KV heads for MQA in all GPUs, so the KV cache size for MQA became equal to the GQA and', metadata={'page': 47, 'source': 'Data/LLAMA_2.pdf'}),\n",
       " Document(page_content='Llama 2 is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has notcovered, nor could it coverall scenarios. For these reasons, aswith all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances\\nproduceinaccurateorobjectionableresponsestouserprompts. Therefore,beforedeployingany\\napplications of Llama 2, developers should perform safety testing and tuning tailored to their', metadata={'page': 76, 'source': 'Data/LLAMA_2.pdf'}),\n",
       " Document(page_content='performed on third-party cloud compute.\\nCarbon Footprint Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware\\nof type A100-80GB (TDP of 350-400W). Estimated total emissions were 539\\ntCO 2eq, 100% of which were offset by Meta’s sustainability program.\\nTraining Data (Sections 2.1 and 3)\\nOverview Llama 2 was pretrained on 2 trillion tokens of data from publicly available\\nsources. The fine-tuning data includes publicly available instruction datasets, as', metadata={'page': 76, 'source': 'Data/LLAMA_2.pdf'})]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "model_path=\"C:\\\\Users\\\\rouna\\\\Desktop\\\\OpenSource_Model\\\\mistral-7b-v0.1.Q8_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from C:\\Users\\rouna\\Desktop\\OpenSource_Model\\mistral-7b-v0.1.Q8_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7338.64 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '7', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 30  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    temperature=0.1,\n",
    "    repeat_penalty=1.5,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=8192,  # Context window size\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Give me architecture of Comparison between GPT3 and LLAMA2?\"\n",
    "context=Final_Retriever.invoke(question)\n",
    "\n",
    "\n",
    "prompt_template=f\"\"\"\n",
    "You are a chatbot that answer question mentioned in the end of the Prompt using Steps and Instruction given below\n",
    "\n",
    "Steps:\n",
    "1. First Understand the question.\n",
    "2. Then find out the texts from the context which can asnwer the question.\n",
    "3. Construct a detail asnwer from the Context given below\n",
    "4. Summarize the answer and into a final answer and provide that as anwer nothing else\n",
    "\n",
    "Instruction:\n",
    "1. Don't give steps you done in between only give the final answer which you summarized\n",
    "2. Always try to give elaborate answer\n",
    "3. If possible make bullet points\n",
    "\n",
    "Context:{context}\n",
    "\n",
    "Question:{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a chatbot that answer question mentioned in the end of the Prompt using Steps and Instruction given below\n",
      "\n",
      "Steps:\n",
      "1. First Understand the question.\n",
      "2. Then find out the texts from the context which can asnwer the question.\n",
      "3. Construct a detail asnwer from the Context given below\n",
      "4. Summarize the answer and into a final answer and provide that as anwer nothing else\n",
      "\n",
      "Instruction:\n",
      "1. Don't give steps you done in between only give the final answer which you summarized\n",
      "2. Always try to give elaborate answer\n",
      "3. If possible make bullet points\n",
      "\n",
      "Context:[Document(page_content='(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyfine-tunedtoalignwithhuman', metadata={'page': 2, 'source': 'Data/LLAMA_2.pdf'}), Document(page_content='distribution, recovering strong performance in the few-shot setting.\\nOn Natural Questions (NQs) GPT-3 achieves 14.6% in the zero-shot setting, 23.0% in the one-shot setting, and 29.9% in\\nthe few-shot setting, compared to 36.6% for ﬁne-tuned T5 11B+SSM. Similar to WebQS, the large gain from zero-shot\\nto few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to', metadata={'page': 13, 'source': 'Data/GPT3.pdf'}), Document(page_content='performed on third-party cloud compute.\\nCarbon Footprint Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware\\nof type A100-80GB (TDP of 350-400W). Estimated total emissions were 539\\ntCO 2eq, 100% of which were offset by Meta’s sustainability program.\\nTraining Data (Sections 2.1 and 3)\\nOverview Llama 2 was pretrained on 2 trillion tokens of data from publicly available\\nsources. The fine-tuning data includes publicly available instruction datasets, as', metadata={'page': 76, 'source': 'Data/LLAMA_2.pdf'}), Document(page_content='Llama 2 is a new technology that carries risks with use. Testing conducted to date has been in\\nEnglish, and has notcovered, nor could it coverall scenarios. For these reasons, aswith all LLMs,\\nLlama 2’s potential outputs cannot be predicted in advance, and the model may in some instances\\nproduceinaccurateorobjectionableresponsestouserprompts. Therefore,beforedeployingany\\napplications of Llama 2, developers should perform safety testing and tuning tailored to their', metadata={'page': 76, 'source': 'Data/LLAMA_2.pdf'}), Document(page_content='GPT-3 Few-Shot 71.8 76.4 75.6 52.0 92.0 69.0\\nWiC WSC MultiRC MultiRC ReCoRD ReCoRD\\nAccuracy Accuracy Accuracy F1a Accuracy F1\\nFine-tuned SOTA 76.1 93.8 62.3 88.2 92.5 93.3\\nFine-tuned BERT-Large 69.6 64.6 24.1 70.0 71.3 72.0\\nGPT-3 Few-Shot 49.4 80.1 30.5 75.4 90.2 91.1\\nTable 3.8: Performance of GPT-3 on SuperGLUE compared to ﬁne-tuned baselines and SOTA. All results are reported', metadata={'page': 18, 'source': 'Data/GPT3.pdf'})]\n",
      "\n",
      "Question:Give me architecture of Comparison between GPT3 and LLAMA2?\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The comparison is based on the following factors:- Architecture- The architectures are different in terms of their size as well as number\\nof layers used for training. In case of LLama 1, it has a total of around \\u00a735 billion parameters,\\nto be precise.\\nit uses an architecture that is similar to GPT but with some modifications such as the use of larger hidden states and attention heads which makes\\nLLAMA more efficient in terms of performance. In case of Llama 2, it has a total number \\u03bcmore parameters than LLAM1.\\nit uses an architecture that is similar to GPT but with some modifications such as the use of larger hidden states and attention heads which makes\\nLLAMA more efficient in terms of performance. In case of Llama 2, it has a total number \\u03bcmore parameters than LLAM1.\\nit uses an architecture that is similar to GPT but with some modifications such as the use of larger hidden states and attention heads which makes\\nLLAMA more efficient in terms of performance. In case of Llama 2, it has a total number \\u03bcmore"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   49816.71 ms\n",
      "llama_print_timings:      sample time =      83.84 ms /   256 runs   (    0.33 ms per token,  3053.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =  107180.70 ms /  1101 tokens (   97.35 ms per token,    10.27 tokens per second)\n",
      "llama_print_timings:        eval time =  115268.58 ms /   255 runs   (  452.03 ms per token,     2.21 tokens per second)\n",
      "llama_print_timings:       total time =  223306.08 ms /  1356 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The comparison is based on the following factors:- Architecture- The architectures are different in terms of their size as well as number\\\\nof layers used for training. In case of LLama 1, it has a total of around \\\\u00a735 billion parameters,\\\\nto be precise.\\\\nit uses an architecture that is similar to GPT but with some modifications such as the use of larger hidden states and attention heads which makes\\\\nLLAMA more efficient in terms of performance. In case of Llama 2, it has a total number \\\\u03bcmore parameters than LLAM1.\\\\nit uses an architecture that is similar to GPT but with some modifications such as the use of larger hidden states and attention heads which makes\\\\nLLAMA more efficient in terms of performance. In case of Llama 2, it has a total number \\\\u03bcmore parameters than LLAM1.\\\\nit uses an architecture that is similar to GPT but with some modifications such as the use of larger hidden states and attention heads which makes\\\\nLLAMA more efficient in terms of performance. In case of Llama 2, it has a total number \\\\u03bcmore'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
